#! /bin/sh 

############################################################################################################################
# SCENARIO 12 - RANDOM STAT() ON FILES -MULTIPLE PROCESSES
############################################################################################################################
# 12.01 OBJECTIVE
#  To understand the file system performance of a random readdir() or stat() call with a large number of files in a large number of directories with multiple processes.
# 12.02 DESCRIPTION
#  Multiple processes performing a random readdir() or stat() with ten (10) million files in ten (10) directories with one (1) million files per directory.
#  It is suggested that a progression of processes in powers of two (2), from two (2) to five hundred twelve (512), be used.
#  Method for running the Scenario:
#   1. Create file system, mount, and tune.
#   2. Randomly make ten (10) directories.
#   3. Randomly create in parallel one (1) million files in each of the ten (10) directories using at least one hundred (100) creation threads or processes.
#   4. Unmount file system.
#   5. Mount file system.
#   6. Start time counter.
#   7. Parallel tree walk using readdir() or stat() walking in order for each of the directories using two (2) walking threads or processes (this is the cold cache test).
#   8. Stop time counter.
#   9. Start time counter.
#   10. Parallel tree walk using readdir() or stat() walking in order for each of the directories using two (2) walking threads or processes (this is the warm cache test).
#   11. Stop time counter.
#  12. Repeat steps 4-11 for parallel processes doing this random action from in powers of two (2) from four (4) to five hundred twelve (512) threads or processes.
# 12.03 EXPECTED RESULTS
#  No measurement is required but the performance numbers should be reported along with the test setup and 
#  at least three (3) runs using cold cache and three (3) runs using warm cache for each of the thread or process counts. 
#  The variance in reported performance should be low especially for the warm cache results.
############################################################################################################################

#PBS -q batch
#PBS -A stf006
#PBS -N S12
#PBS -j oe
#PBS -o ./results_Scenario_12_Random_Stat_Many_Files_Multple_Procs.$PBS_JOBID
#PBS -l walltime=0:20:00,size=160

cd $PBS_O_WORKDIR
EXECUTABLE="./mdtest"

##############################################################################
########## Change parameters below as needed #################################
########## Note: task == process            ##################################
TPN=4                                    # Tasks per Node
FPT=64                                   # Files per task
ITERS=1                                  # Number of times to run test
VERB="     "                             # Verbose mode "-v -v"
PRE_CREATE=0                             # If true, create 0-length files as specified below accross OSTs
tests="1000"                           # specify target number of files to create in each directory 
dir_list="09 01 04 03 02 06 05 07 08 00" # list of dir names
############# Should not have to change stuff below this line #################

#################################
# get dir count
#################################
let "dircount = 0"
for dir in $dir_list
do
let "dircount = dircount + 1"
done

for nfiles in $tests
do

#################################
# Calculate Per Directory quants
#################################
let "nprocs = $nfiles / $FPT"
let "nodes  = $nprocs / $TPN + 1" # round up to nearest # nodes
let "nprocs = $nodes  * $TPN"     # actual # procs
let "mfiles = $nprocs * $FPT"     # actual # files
echo "Starting tests on $dircount directories. Per directory resources: $nodes nodes, $nprocs processes, $mfiles files"

##########################################################################################################
## 2. Randomly make ten (10) directories. 
##         How are directories made randomly??? Humor the spec writers...
##########################################################################################################
for dir in $dir_list
do
rm -fr ${dir}
mkdir  ${dir}
###########################################################################################
# create 0 length files,  stripe 1, round robin files accross MAX_OSTs OSTs. tune to suit.
###########################################################################################
if [ "$PRE_CREATE" == "1" ]
then
DATA_FILE="mdtest"
date
iost=0
iproc=0
while [ "$iproc" -lt "$nprocs" ]
do
ifile=0
while [ "$ifile" -lt "$FPT" ]
do
lfs setstripe -s 1m -c 1 -i $iost ${dir}/${DATA_FILE}.${iproc}.${ifile}
iost=`expr $iost + 1`
ifile=`expr $ifile + 1`
done
iproc=`expr $iproc + 1`
done
let "$ifile = $nprocs * $FPT"
echo "PRE-CREATED $ifile 0-len files"
fi

date
##########################################################################################################
## 3. Randomly create in parallel one (1) million files in each of the ten (10) directories 
##    using at least one hundred (100) creation threads or processes.
##         OK, Let's create a bunch with random sizes, and then random stat 'em. And we do it with 
##         # of directory parallel launches of mdtest. This ought to scramble things a bit!
##########################################################################################################
# File create, random sizes between 1-64KB, concurrent random stat tests, files only, no directories, $FPT files per task/process
aprun -n ${nprocs} -N $TPN $EXECUTABLE -d ${dir} -n $FPT -i $ITERS  -F -w 1024 -W 65536  -R 0 -k   $VERB &
done
wait

###########################################
# done with file creation. build ordered list
# Randomly shuffle ordered list
###########################################
date
./rshuffled $dir_list > file_list_random
##########################################################################################################
##   This is the cold cache case
## 6. Start time counter.
## 7. Serial random walk using readdir() or stat() of all the files in each of the directories in a random order 
##    using only one (1) thread or process (this is the cold cache test).
##    OK, the spec said one (1) process!!!!!
## 8. Stop time counter.
##########################################################################################################
let "mfiles = $mfiles * $dircount"
# multiple process stat() of all files on randomly shuffled file of $mfiles files...dont remove files 
for iprocs in "2 4 8 16 32 64 128 256 512"
do
echo "Started tests on $dircount directories. Per directory resources: $iprocs processes, $mfiles files"
date
aprun -n $iprocs -N 1 $EXECUTABLE -d . -n $mfiles -i $ITERS  -F -L file_list_random -B 0 -E -k   $VERB 
##########################################################################################################
##   This is the warm cache case
##########################################################################################################
date
aprun -n $iprocs -N $TPN $EXECUTABLE -d . -n $mfiles -i $ITERS  -F -L file_list_random -B 0 -E -k   $VERB 

echo "Finished tests on $dircount directories. Per directory resources: $iprocs processes, $mfiles files"
done

#date
#lfs getstripe ${DATA_DIR}/*

#####################################################################################################
##     remove files in parallel
######################################################################################################
for dir in $dir_list
do
aprun -n ${nprocs} -N $TPN $EXECUTABLE -d ${dir} -n $FPT -i $ITERS  -F -w 1024 -W 65536  -R 0 -r   $VERB &
done
wait
for dir in $dir_list
do
rmdir ${dir}
done

done
